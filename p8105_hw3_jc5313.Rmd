---
title: "Homework 3"
author: "Juan Cambeiro"
date: "2022-10-17"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

### Problem 2

#### Load, clean, tidy, mutate

Here I am loading the accelerometer data, cleaning it, and tidying it using `pivot_longer`. Then, I am mutating the data using `mutate` so as to create a new 'weekday_vs_weekend' variable and to encode data with reasonable variable classes.
```{r}
accel_data = read_csv("data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes",
    values_to = "activity_count",
    names_prefix = "activity_") %>%
  mutate(
  weekday_vs_weekend = as.factor(ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday")),
  day = as.factor(ordered(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))),
  minutes = as.double(minutes))
```
Description: This dataset contains `r nrow(accel_data)` observations and `r ncol(accel_data)` variables. Each observation is a minute of accelerometer data for the 63 year-old male over five weeks. The variables include: week (`week`), day ID (`day_id`), day of the week (`day`), minute (`minutes`), activity count per minute (`activity_count`), and whether it is on a weekday or weekend (`weekday_vs_weekend`).

#### Aggregating across minutes to create total activity variable

I will now use my tidied dataset to aggregate across minutes to create a total activity variable for each day, and create a table showing these totals.
```{r}
accel_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) %>%
  pivot_wider(
    names_from = "day",
    values_from = "total_activity") %>%
  knitr::kable()
```
Above we can see a table showing the totals for each day.

#### Making plot that shows the 24-hour activity time courses for each day

I will now make a single-panel plot that shows the 24-hour activity time courses for each day.
```{r}
accel_data  %>%
  mutate(
  hour = as.numeric(rep(rep(seq(1,24), each = 60), 35))) %>%
  group_by(week, day, hour) %>%
  summarize(total_activity = sum(activity_count)) %>%
  ggplot(aes (x = hour, y = total_activity, color = day)) +
  geom_point() +
  geom_line() +
  labs(title = "24 hour activity time courses")
```
In the above plot it is clear that activity levels are low in the early hours of the morning, presumably since this is when people are sleeping. Activity levels seem to be highest in the evening around 9PM. Activity levels do not seem to vary all that much by day of the week.

### Problem 3

#### Load data

First, I am loading the NOAA data.
```{r}
library(p8105.datasets)
data("ny_noaa")
```
Description: This dataset contains `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. The variables include: weather station id (`id`), date of observation (`date`), precipitation (`prcp`), snowfall (`snow`), snow depth (`snwd`), maximum temperature (`tmax`), and minimum temperature (`tmin`). Missing data seems to be a significant issue: for precipitation (`prcp`) there are `r sum(is.na(ny_noaa$prcp))` missing values, for snowfall (`snow`) there are `r sum(is.na(ny_noaa$snow))` missing values, for snow depth (`snwd`) there are `r sum(is.na(ny_noaa$snwd))` missing values, for maximum temperature (`tmax`) there are `r sum(is.na(ny_noaa$tmax))` missing values, and for minimum temperature (`tmin`) there are `r sum(is.na(ny_noaa$tmin))` missing values.

#### Data cleaning
Next, I am cleaning the data and I will create separate variables for year, month, and day using `separate`. I am also using `mutate` to encode data with reasonable variable classes and units.
```{r}
ny_noaa_cleaned = ny_noaa %>%
  janitor::clean_names() %>%
  separate (col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.factor(year),
    month = as.factor(month),
    tmin = as.numeric(tmin),
    tmax = as.numeric(tmax))
```

#### Most commonly observed value(s) for snowfall
Next, I will find the most commonly observed values for snowfall.
```{r}
ny_noaa_cleaned %>%
  group_by(snow) %>%
  summarize(snowfall = n()) %>%
  arrange(desc(snowfall))
```
The most commonly observed value for snowfall is 0 because most days it does not snow at all. The second most common observed value is NA because there is a substantial number of missing values.

#### Plot showing the average max temperature in January and in July in each station across years
Now, I will do a two-panel plot showing the average max temperature in January and in July in each station across years.
```{r}
avg_jan = ny_noaa_cleaned %>%
  filter(month == "1") %>%
  filter(!is.na(tmax)) %>%
  group_by(id, year) %>%
  summarize(avg_tmax = mean(tmax)) %>%
  ggplot(aes(x=year, y=avg_tmax, color=id)) +
  geom_path() +
  theme(legend.position = "none") +
  labs (title = "avg max temp in Jan over the years",
        x = "year",
        y = "avg max temperature")
avg_july = ny_noaa_cleaned %>%
  filter(month == "7") %>%
  filter(!is.na(tmax)) %>%
  group_by(id, year) %>%
  summarize(avg_tmax = mean(tmax)) %>%
  ggplot(aes(x=year, y=avg_tmax, color=id)) +
  geom_path() +
  theme(legend.position = "none") +
  labs (title = "avg max temp in July over the years",
        x = "year",
        y = "avg max temperature")
(avg_jan + avg_july)
```

####  Plot showing tmax vs tmin for the full dataset 
```{r}
tmax_vs_tmin = ny_noaa_cleaned %>%
  filter(!is.na(tmax), !is.na(tmin)) %>%
  ggplot(aes (x=tmin, y=tmax)) +
  geom_hex() +
  labs(title = "temp tmax vs tmin")
```

####  Plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.






